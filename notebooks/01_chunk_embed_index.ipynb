{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¢ Notebook 01: Chunking, Embedding, and Vector Store Indexing (FAISS)\n",
    "\n",
    "## Learning Objectives\n",
    "In this notebook, you will learn:\n",
    "1. **Convert data to LangChain Documents** - the standard format for RAG\n",
    "2. **Chunk text** using RecursiveCharacterTextSplitter\n",
    "3. **Create embeddings** using sentence-transformers/all-MiniLM-L6-v2\n",
    "4. **Build a FAISS vector store** and persist it to disk\n",
    "5. **Test retrieval** with sample queries\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is Chunking?\n",
    "- Large documents need to be split into smaller pieces (chunks)\n",
    "- Embedding models have input limits (typically 256-512 tokens)\n",
    "- Smaller chunks = more precise retrieval, but less context\n",
    "- We use **overlap** to preserve context at chunk boundaries\n",
    "\n",
    "### What are Embeddings?\n",
    "- Embeddings convert text to vectors (lists of numbers)\n",
    "- Similar texts have similar vectors (close in vector space)\n",
    "- This enables **semantic search** (meaning-based, not just keywords)\n",
    "\n",
    "### What is a Vector Store?\n",
    "- A system for storing and searching vectors\n",
    "- **FAISS** is a fast similarity search library (fully local)\n",
    "- We can save it to disk and reload later (no re-embedding needed!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ HuggingFace cache set to: /Users/macbookpro/Documents/Rag_Test/rag-ticket-rag/models/hf\n",
      "âœ“ Setup complete!\n",
      "Project root: /Users/macbookpro/Documents/Rag_Test/rag-ticket-rag\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# IMPORTANT: Set up HuggingFace cache BEFORE importing transformers\n",
    "# This ensures models are downloaded to our project folder\n",
    "from src.config import setup_hf_cache\n",
    "setup_hf_cache()\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "print(\"âœ“ Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/Documents/Rag_Test/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/macbookpro/Documents/Rag_Test/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/macbookpro/Documents/Rag_Test/.venv/lib/python3.14/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom modules imported!\n"
     ]
    }
   ],
   "source": [
    "# Import our custom modules\n",
    "from src import config\n",
    "from src.io import load_processed_tickets\n",
    "from src.docs import dataframe_to_documents, print_document_sample\n",
    "from src.chunking import chunk_documents, get_chunk_stats, print_chunk_samples\n",
    "from src.vectorstore import (\n",
    "    create_vector_store,\n",
    "    load_vector_store,\n",
    "    search_similar,\n",
    "    search_with_scores,\n",
    "    print_search_results\n",
    ")\n",
    "\n",
    "print(\"âœ“ Custom modules imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Cleaned Data\n",
    "\n",
    "We'll load the preprocessed data from Notebook 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 8,469 processed tickets from tickets_clean.csv\n",
      "\n",
      "Loaded 8,469 tickets\n",
      "Columns: ['Ticket ID', 'Customer Age', 'Customer Gender', 'Product Purchased', 'Date of Purchase', 'Ticket Type', 'Ticket Subject', 'Ticket Description', 'Ticket Status', 'Resolution', 'Ticket Priority', 'Ticket Channel', 'First Response Time', 'Time to Resolution', 'Customer Satisfaction Rating', 'description_word_count', 'document_text']\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned tickets data\n",
    "df = load_processed_tickets()\n",
    "\n",
    "# Quick preview\n",
    "print(f\"\\nLoaded {len(df):,} tickets\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document_text:\n",
      "--------------------------------------------------\n",
      "Subject: Product setup\n",
      "Description: I'm having an issue with the {product_purchased}. Please assist.\n",
      "\n",
      "Your billing zip code is: 71701.\n",
      "\n",
      "We appreciate that you have requested a website address.\n",
      "\n",
      "Please double check your email address. I've tried troubleshooting steps mentioned in the user manual, but the issue persists.\n"
     ]
    }
   ],
   "source": [
    "# Verify document_text exists\n",
    "assert 'document_text' in df.columns, \"document_text column missing! Run Notebook 00 first.\"\n",
    "\n",
    "# Show sample document_text\n",
    "print(\"Sample document_text:\")\n",
    "print(\"-\" * 50)\n",
    "print(df.iloc[0]['document_text'][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Convert to LangChain Documents\n",
    "\n",
    "LangChain uses `Document` objects as a standard format:\n",
    "- `page_content`: The text that gets embedded\n",
    "- `metadata`: Additional info for filtering/display (not embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Converted 8,469 rows to LangChain Documents\n",
      "  Sample metadata keys: ['ticket_id', 'product', 'ticket_type', 'ticket_subject', 'status', 'priority', 'channel', 'date_of_purchase', 'satisfaction_rating']\n",
      "\n",
      "Created 8,469 documents\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame rows to LangChain Documents\n",
    "documents = dataframe_to_documents(df)\n",
    "\n",
    "print(f\"\\nCreated {len(documents):,} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOCUMENT SAMPLE\n",
      "============================================================\n",
      "Content:\n",
      "Subject: Product setup\n",
      "Description: I'm having an issue with the {product_purchased}. Please assist.\n",
      "\n",
      "Your billing zip code is: 71701.\n",
      "\n",
      "We appreciate that you have requested a website address.\n",
      "\n",
      "Please...\n",
      "------------------------------------------------------------\n",
      "Metadata:\n",
      "  ticket_id: 1\n",
      "  product: GoPro Hero\n",
      "  ticket_type: Technical issue\n",
      "  ticket_subject: Product setup\n",
      "  status: Pending Customer Response\n",
      "  priority: Critical\n",
      "  channel: Social media\n",
      "  date_of_purchase: 2021-03-22\n",
      "  satisfaction_rating: None\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's examine a sample document\n",
    "print_document_sample(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:**\n",
    "- `page_content` contains the text we'll embed (Subject + Description + Resolution)\n",
    "- `metadata` contains useful info like ticket_id, product, priority\n",
    "- Metadata is NOT embedded, but stored alongside for filtering and display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Chunk the Documents\n",
    "\n",
    "We'll split documents into smaller chunks using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "### Parameters:\n",
    "- **chunk_size=500**: Maximum 500 characters per chunk\n",
    "- **chunk_overlap=50**: 50 characters overlap between consecutive chunks\n",
    "\n",
    "### Why these values?\n",
    "- 500 chars â‰ˆ 100-125 words â‰ˆ good balance of context and precision\n",
    "- 50 char overlap (10%) helps preserve context at boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Configuration:\n",
      "  chunk_size: 500 characters\n",
      "  chunk_overlap: 50 characters\n"
     ]
    }
   ],
   "source": [
    "# Show current config settings\n",
    "print(\"Chunking Configuration:\")\n",
    "print(f\"  chunk_size: {config.CHUNK_SIZE} characters\")\n",
    "print(f\"  chunk_overlap: {config.CHUNK_OVERLAP} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created text splitter (chunk_size=500, overlap=50)\n",
      "âœ“ Chunking complete:\n",
      "  Original documents: 8,469\n",
      "  After chunking: 8,469\n",
      "  Expansion ratio: 1.00x\n"
     ]
    }
   ],
   "source": [
    "# Chunk the documents\n",
    "chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHUNK STATISTICS\n",
      "========================================\n",
      "total_chunks: 8469\n",
      "min_length: 183\n",
      "max_length: 487\n",
      "mean_length: 344.4\n",
      "median_length: 348\n"
     ]
    }
   ],
   "source": [
    "# Get statistics about the chunks\n",
    "stats = get_chunk_stats(chunks)\n",
    "\n",
    "print(\"\\nCHUNK STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHUNK SAMPLES (showing 2 of 8,469)\n",
      "============================================================\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Length: 320 chars\n",
      "Ticket ID: 1\n",
      "Chunk Index: 0\n",
      "Content preview:\n",
      "Subject: Product setup\n",
      "Description: I'm having an issue with the {product_purchased}. Please assist.\n",
      "\n",
      "Your billing zip code is: 71701.\n",
      "\n",
      "We appreciate that you have requested a website address.\n",
      "\n",
      "Please...\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Length: 329 chars\n",
      "Ticket ID: 2\n",
      "Chunk Index: 0\n",
      "Content preview:\n",
      "Subject: Peripheral compatibility\n",
      "Description: I'm having an issue with the {product_purchased}. Please assist.\n",
      "\n",
      "If you need to change an existing product.\n",
      "\n",
      "I'm having an issue with the {product_purch...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sample chunks to see what they look like\n",
    "print_chunk_samples(chunks, n_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "- Each chunk has the original metadata (ticket_id, product, etc.)\n",
    "- A `chunk_index` was added to track which chunk of a ticket this is\n",
    "- Short tickets stay as one chunk; longer ones are split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Create Embeddings and Vector Store (FAISS)\n",
    "\n",
    "Now we'll:\n",
    "1. Load the embedding model (all-MiniLM-L6-v2)\n",
    "2. Embed all chunks (convert text â†’ vectors)\n",
    "3. Store in **FAISS**\n",
    "4. Persist to disk (saved under `vector_store/faiss/`)\n",
    "\n",
    "**Note:** First run will download the embedding model (~80MB). This is cached for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Model Configuration:\n",
      "  Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Cache directory: /Users/macbookpro/Documents/Rag_Test/rag-ticket-rag/models/hf\n",
      "  Vector store directory: /Users/macbookpro/Documents/Rag_Test/rag-ticket-rag/vector_store/faiss\n"
     ]
    }
   ],
   "source": [
    "# Show embedding model info\n",
    "print(\"Embedding Model Configuration:\")\n",
    "print(f\"  Model: {config.EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"  Cache directory: {config.MODELS_DIR}\")\n",
    "print(f\"  Vector store directory: {config.VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store... (this may take a few minutes)\n",
      "==================================================\n",
      "Creating vector store with 8,469 documents...\n",
      "  Persist directory: /Users/macbookpro/Documents/Rag_Test/rag-ticket-rag/vector_store/faiss\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  (First run will download ~80MB to /Users/macbookpro/Documents/Rag_Test/rag-ticket-rag/models/hf)\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store\n",
    "# This will:\n",
    "# 1. Download the embedding model (first time only)\n",
    "# 2. Embed all chunks (may take a few minutes)\n",
    "# 3. Store in FAISS\n",
    "# 4. Persist to disk\n",
    "\n",
    "print(\"Creating FAISS vector store... (this may take a few minutes)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "vectorstore = create_vector_store(chunks)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ“ FAISS vector store created and persisted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Test Loading from Disk\n",
    "\n",
    "Let's verify we can reload the vector store from disk.\n",
    "This is important - we don't want to re-embed every time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the current vectorstore from memory\n",
    "del vectorstore\n",
    "\n",
    "# Reload from disk\n",
    "print(\"Reloading vector store from disk...\")\n",
    "vectorstore = load_vector_store()\n",
    "\n",
    "print(\"\\nâœ“ Successfully reloaded from disk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Test Retrieval\n",
    "\n",
    "Let's test the vector store with some sample queries.\n",
    "This demonstrates **semantic search** - finding relevant documents by meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    \"billing refund issues\",\n",
    "    \"device overheating problem\",\n",
    "    \"late delivery or missing items\",\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(test_queries)} queries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 1: Billing refund issues\n",
    "query = test_queries[0]\n",
    "results = search_similar(vectorstore, query, k=3)\n",
    "print_search_results(results, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 2: Device overheating\n",
    "query = test_queries[1]\n",
    "results = search_similar(vectorstore, query, k=3)\n",
    "print_search_results(results, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 3: Late delivery\n",
    "query = test_queries[2]\n",
    "results = search_similar(vectorstore, query, k=3)\n",
    "print_search_results(results, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with Similarity Scores\n",
    "\n",
    "We can also get similarity scores to understand how relevant each result is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with scores\n",
    "query = \"customer wants a refund for damaged product\"\n",
    "results_with_scores = search_with_scores(vectorstore, query, k=5)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nResults with similarity scores:\")\n",
    "print(\"(Lower score = more similar)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (doc, score) in enumerate(results_with_scores, 1):\n",
    "    print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "    print(f\"   Ticket ID: {doc.metadata.get('ticket_id', 'N/A')}\")\n",
    "    print(f\"   Product: {doc.metadata.get('product', 'N/A')}\")\n",
    "    print(f\"   Preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Explore the Vector Store (FAISS)\n",
    "\n",
    "FAISS stores vectors in an index.\n",
    "We can inspect the number of vectors indexed and what files were saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect FAISS index size\n",
    "print(\"VECTOR STORE INFO (FAISS)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ntotal = number of vectors stored in the FAISS index\n",
    "try:\n",
    "    print(f\"Total vectors (ntotal): {int(vectorstore.index.ntotal):,}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not read FAISS ntotal: {e}\")\n",
    "\n",
    "print(f\"Persist directory: {config.VECTOR_STORE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the persisted files on disk\n",
    "from pathlib import Path\n",
    "\n",
    "persist_dir = Path(config.VECTOR_STORE_DIR)\n",
    "files = sorted([p.name for p in persist_dir.glob(\"*\")])\n",
    "\n",
    "print(\"\\nPERSISTED FILES\")\n",
    "print(\"=\" * 40)\n",
    "for f in files:\n",
    "    print(\"-\", f)\n",
    "\n",
    "print(\"\\nExpected FAISS files:\")\n",
    "print(\"- index.faiss  (the vector index)\")\n",
    "print(\"- index.pkl    (docstore + metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "1. âœ… Loaded cleaned ticket data from Notebook 00\n",
    "2. âœ… Converted DataFrame rows to LangChain Document objects\n",
    "3. âœ… Chunked documents using RecursiveCharacterTextSplitter (500 chars, 50 overlap)\n",
    "4. âœ… Created embeddings using all-MiniLM-L6-v2\n",
    "5. âœ… Built and persisted a **FAISS** vector store\n",
    "6. âœ… Tested retrieval with sample queries\n",
    "\n",
    "### Key Takeaways\n",
    "- **Chunking** splits large documents into searchable pieces\n",
    "- **Embeddings** convert text to vectors for semantic search\n",
    "- **FAISS** enables fast similarity search locally\n",
    "- **Persistence** means we don't re-embed every time\n",
    "\n",
    "### Files Created\n",
    "- `vector_store/faiss/` - Persisted FAISS index\n",
    "- `models/hf/` - Cached embedding model\n",
    "\n",
    "### Next Steps\n",
    "â†’ **Notebook 02**: Build the full RAG pipeline with LLM generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ Notebook 01 Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nVector store saved to: {config.VECTOR_STORE_DIR}\")\n",
    "try:\n",
    "    print(f\"Total vectors indexed (ntotal): {int(vectorstore.index.ntotal):,}\")\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"\\nProceed to: 02_build_rag_pipeline.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
